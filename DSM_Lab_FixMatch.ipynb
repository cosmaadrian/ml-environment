{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNe7vTnb5NU5eHd9T4icvAm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cosmaadrian/ml-environment/blob/master/DSM_Lab_FixMatch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DSM Lab 6\n",
        "## Semi-Supervised Learning with FixMatch\n",
        "\n",
        "In this lab, we will explore the implementation of FixMatch, a method for performing semi-supervised learning.\n",
        "\n",
        "We will perform our training on a fraction of CIFAR10, a dataset of natural images\n",
        "\n",
        "You can find the original paper here: FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence (https://arxiv.org/abs/2001.07685)\n",
        "\n"
      ],
      "metadata": {
        "id": "LIfzH1M7IZMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.models import resnet18\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import RandomSampler, SequentialSampler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "uxxtw2ThIrZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train_epoch(model, dataloader, device, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "\n",
        "    total_train_loss = 0.0\n",
        "    dataset_size = 0\n",
        "\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader), colour='cyan', file=sys.stdout)\n",
        "    for step, (images, labels) in bar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        batch_size = images.shape[0]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(images)\n",
        "        loss = criterion(pred, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = np.round(total_train_loss / dataset_size, 2)\n",
        "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss)\n",
        "\n",
        "\n",
        "    return epoch_loss\n",
        "\n",
        "def valid_epoch(model, dataloader, device, criterion, epoch):\n",
        "    model.eval()\n",
        "\n",
        "    total_val_loss = 0.0\n",
        "    dataset_size = 0\n",
        "\n",
        "    correct = 0\n",
        "\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader), colour='cyan', file=sys.stdout)\n",
        "    for step, (images, labels) in bar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        batch_size = images.shape[0]\n",
        "\n",
        "        pred = model(images)\n",
        "        loss = criterion(pred, labels)\n",
        "\n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        total_val_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = np.round(total_val_loss / dataset_size, 2)\n",
        "\n",
        "        accuracy = np.round(100 * correct / dataset_size, 2)\n",
        "\n",
        "        bar.set_postfix(Epoch=epoch, Valid_Acc=accuracy, Valid_Loss=epoch_loss)\n",
        "\n",
        "    return accuracy, epoch_loss\n",
        "\n",
        "def run_training(model, trainloader, testloader, criterion, optimizer, num_epochs):\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
        "\n",
        "    top_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train_loss = train_epoch(model, trainloader, device, optimizer, criterion, epoch)\n",
        "        with torch.no_grad():\n",
        "            val_accuracy, val_loss = valid_epoch(model, testloader, device, criterion, epoch)\n",
        "            if val_accuracy > top_accuracy:\n",
        "                print(f\"Validation Accuracy Improved ({top_accuracy} ---> {val_accuracy})\")\n",
        "                top_accuracy = val_accuracy\n",
        "        print()"
      ],
      "metadata": {
        "id": "RVVPuvbzIteq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't touch this for now.\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "cifar_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "cifar_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Let's just use 10% of the data to make it harder\n",
        "from collections import defaultdict\n",
        "indices_per_class = defaultdict(list)\n",
        "for i in range(len(cifar_trainset)):\n",
        "  _, class_label = cifar_trainset[i]\n",
        "  indices_per_class[class_label].append(i)\n",
        "\n",
        "labeled_indices = []\n",
        "unlabeled_indices = []\n",
        "for class_name, indices in indices_per_class.items():\n",
        "  labeled_indices.extend(indices[:int(0.1 * len(indices))]) # 10% labeled\n",
        "  unlabeled_indices.extend(indices[int(0.1 * len(indices)):]) # 90% unlabeled\n",
        "\n",
        "cifar_labeled_trainset = torch.utils.data.Subset(dataset = cifar_trainset, indices = labeled_indices)\n",
        "\n",
        "cifar_labeled_trainloader = DataLoader(cifar_labeled_trainset, batch_size=64, shuffle=True)\n",
        "cifar_testloader = DataLoader(cifar_testset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dB2K6AkoIyLd",
        "outputId": "c6c89463-0fc4-4237-dc03-efb125ee27e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline: Training just with the supervised data."
      ],
      "metadata": {
        "id": "n30DeNSgKQfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained = False) # let's initialize a ResNet18 from scratch and pretrain it ourselves\n",
        "model.fc = nn.Linear(in_features=model.fc.in_features, out_features=10, bias=True)\n",
        "\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam is an improved gradient descent algorithm\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfMgwZewJddY",
        "outputId": "f7499174-b99d-432a-d6f2-16e4ab1214e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_training(model, cifar_labeled_trainloader, cifar_testloader, criterion = criterion, optimizer = optimizer, num_epochs = epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC230FhoJlgF",
        "outputId": "7269423e-26fd-42d3-bb41-adea42033fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using GPU: Tesla T4\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 79/79 [00:05<00:00, 14.59it/s, Epoch=0, Train_Loss=2.03]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 36.39it/s, Epoch=0, Valid_Acc=35.8, Valid_Loss=1.78]\n",
            "Validation Accuracy Improved (0.0 ---> 35.83)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 79/79 [00:03<00:00, 24.03it/s, Epoch=1, Train_Loss=1.29]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 32.07it/s, Epoch=1, Valid_Acc=40.1, Valid_Loss=1.69]\n",
            "Validation Accuracy Improved (35.83 ---> 40.11)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 79/79 [00:03<00:00, 24.20it/s, Epoch=2, Train_Loss=0.81]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 36.16it/s, Epoch=2, Valid_Acc=39.8, Valid_Loss=1.73]\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 79/79 [00:03<00:00, 21.33it/s, Epoch=3, Train_Loss=0.44]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 36.41it/s, Epoch=3, Valid_Acc=40.4, Valid_Loss=1.85]\n",
            "Validation Accuracy Improved (40.11 ---> 40.35)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 79/79 [00:03<00:00, 24.61it/s, Epoch=4, Train_Loss=0.24]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 31.85it/s, Epoch=4, Valid_Acc=40.9, Valid_Loss=1.96]\n",
            "Validation Accuracy Improved (40.35 ---> 40.88)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 79/79 [00:03<00:00, 23.26it/s, Epoch=5, Train_Loss=0.12]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 32.65it/s, Epoch=5, Valid_Acc=40.5, Valid_Loss=2.07]\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 79/79 [00:03<00:00, 20.79it/s, Epoch=6, Train_Loss=0.08]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 36.57it/s, Epoch=6, Valid_Acc=40.2, Valid_Loss=2.13]\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 79/79 [00:03<00:00, 24.45it/s, Epoch=7, Train_Loss=0.08]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 31.86it/s, Epoch=7, Valid_Acc=40.5, Valid_Loss=2.26]\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 79/79 [00:03<00:00, 24.17it/s, Epoch=8, Train_Loss=0.15]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 36.17it/s, Epoch=8, Valid_Acc=39.5, Valid_Loss=2.37]\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 79/79 [00:03<00:00, 20.80it/s, Epoch=9, Train_Loss=0.07]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 35.91it/s, Epoch=9, Valid_Acc=39.8, Valid_Loss=2.4]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results suck. Let's make them better.\n",
        "\n",
        "First, let's define a set of weak and strong augmentations."
      ],
      "metadata": {
        "id": "BU9jGYtwKJfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weak_transforms = transforms.Compose([\n",
        "    # TODO add random horizontal flips\n",
        "    # TODO add random crops (use padding=int(32*0.125))\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "]) # weak transforms are just random shifting and flipping\n",
        "\n",
        "strong_transforms = transforms.Compose([\n",
        "    # TODO add random horizontal flips\n",
        "    # TODO add random crops (use padding=int(32*0.125))\n",
        "    # TODO add color jitter\n",
        "    # TODO add random grayscale with some small probability\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "class TransformFixMatch(object):\n",
        "    def __init__(self, weak, strong):\n",
        "        self.weak = weak\n",
        "        self.strong = strong\n",
        "\n",
        "    def __call__(self, x):\n",
        "        weak = self.weak(x)\n",
        "        strong = self.strong(x)\n",
        "\n",
        "        return weak, strong\n",
        "\n",
        "class CIFAR10SSL(datasets.CIFAR10):\n",
        "    def __init__(self, root, indexs, train=True, transform=None, target_transform=None, download=False):\n",
        "        super().__init__(root, train=train, transform=transform, target_transform=target_transform, download=download)\n",
        "        if indexs is not None:\n",
        "            self.data = self.data[indexs]\n",
        "            self.targets = np.array(self.targets)[indexs]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n"
      ],
      "metadata": {
        "id": "xgF5v90TJtzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the new transforms\n",
        "cifar_labeled_trainset = CIFAR10SSL(root='./data', indexs = labeled_indices, train=True, transform = weak_transforms)\n",
        "cifar_unlabeled_trainset = CIFAR10SSL(root='./data', indexs = unlabeled_indices, train=True, transform=TransformFixMatch(weak = weak_transforms, strong = strong_transforms))\n",
        "cifar_testset = datasets.CIFAR10(root='./data', train=False, transform = val_transforms, download=False)\n",
        "\n",
        "# One training batch contains 1/8 labeled data and 7/8 unlabeled data.\n",
        "cifar_labeled_trainloader = DataLoader(cifar_labeled_trainset, batch_size=64, sampler = RandomSampler(cifar_labeled_trainset))\n",
        "cifar_unlabeled_trainloader = DataLoader(cifar_unlabeled_trainset, batch_size=7 * 64, sampler = RandomSampler(cifar_unlabeled_trainset))\n",
        "\n",
        "# Test set is all labeled\n",
        "cifar_testloader = DataLoader(cifar_testset, batch_size=64, shuffle = False)"
      ],
      "metadata": {
        "id": "xiBDV-wfMaL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fixmatch_epoch(model, labeled_dataloader, unlabeled_dataloader, device, optimizer, epoch):\n",
        "    criterion_labeled = nn.CrossEntropyLoss()\n",
        "    criterion_unlabeled = nn.CrossEntropyLoss(reduction='none') # loss per example\n",
        "\n",
        "    # TODO try out some other values here?\n",
        "    threshold = 0.90 # predictions smaller than 90% confidence are filtered.\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total_train_loss = 0.0\n",
        "    dataset_size = 0\n",
        "\n",
        "    bar = tqdm(enumerate(unlabeled_dataloader), total=len(unlabeled_dataloader), colour='cyan', file=sys.stdout)\n",
        "\n",
        "    labeled_iterator = iter(labeled_dataloader)\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for step, (unlabeled_images, _) in bar:\n",
        "        unlabeled_images_weak, unlabeled_images_strong = unlabeled_images\n",
        "\n",
        "        unlabeled_images_weak = unlabeled_images_weak.to(device)\n",
        "        unlabeled_images_strong = unlabeled_images_strong.to(device)\n",
        "        try:\n",
        "          labeled_images, labels = next(labeled_iterator)\n",
        "        except StopIteration as e:\n",
        "          labeled_iterator = iter(labeled_dataloader)\n",
        "          labeled_images, labels = next(labeled_iterator)\n",
        "\n",
        "        labeled_images = labeled_images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred_labeled = model(labeled_images)\n",
        "\n",
        "        # get pseudo-labels, don't propagate gradients\n",
        "        with torch.no_grad():\n",
        "          pred_weak = model(unlabeled_images_weak)\n",
        "\n",
        "          # get confidence as a probability\n",
        "          pred_weak_confidence = torch.nn.functional.softmax(pred_weak, dim = -1)\n",
        "          max_values, max_indices = torch.max(pred_weak_confidence, dim = -1)\n",
        "\n",
        "          # filter out unconfident predictions\n",
        "          fixmatch_mask = (max_values > threshold).float()\n",
        "\n",
        "        # TODO other things to try out\n",
        "        # add mixup between labeled data and unlabeled data\n",
        "        # (interpolate labeled images with strongly augmented unlabeled images and between corresponding true labels and pseudo-labels)\n",
        "        # mixup: BEYOND EMPIRICAL RISK MINIMIZATION https://arxiv.org/pdf/1710.09412\n",
        "\n",
        "        # TODO (more complicated)\n",
        "        # some pseudo-labels might be wrong and stay wrong throughout training\n",
        "        # maybe figure out which ones are wrong by looking at training dynamics\n",
        "        # Identifying Mislabeled Data using the Area Under the Margin Ranking https://arxiv.org/pdf/2001.10528\n",
        "        # MarginMatch: Improving Semi-Supervised Learning with Pseudo-Margins https://arxiv.org/pdf/2308.09037\n",
        "\n",
        "        pred_strong = model(unlabeled_images_strong)\n",
        "\n",
        "        # loss for labeled images (nothing special, crossentropy between true labels and preds)\n",
        "        loss_labeled = criterion_labeled(pred_labeled, labels)\n",
        "\n",
        "        # loss for unlabeled: crossentropy between high-confidence pseudo-labels on weak augmentations and preds on strong augmentations\n",
        "        # fixmatch_mask filters out unconfident pseudo-labels\n",
        "        loss_consistency = criterion_unlabeled(pred_strong, max_indices) * fixmatch_mask\n",
        "        loss_consistency = loss_consistency.mean()\n",
        "\n",
        "        loss = loss_labeled + loss_consistency\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        bar.set_postfix(Epoch=epoch, LabeledLoss=loss_labeled.item(), ConsistencyLoss = loss_consistency.item(), FractionMasked=(1 - fixmatch_mask.float().mean()).item())\n",
        "\n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "def run_training_fixmatch(model, labeled_trainloader, unlabeled_trainloader, testloader, optimizer, num_epochs):\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
        "\n",
        "    top_accuracy = 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train_loss = train_fixmatch_epoch(model, labeled_trainloader, unlabeled_trainloader, device, optimizer, epoch)\n",
        "        with torch.no_grad():\n",
        "            val_accuracy, val_loss = valid_epoch(model, testloader, device, criterion, epoch)\n",
        "            if val_accuracy > top_accuracy:\n",
        "                print(f\"Validation Accuracy Improved ({top_accuracy} ---> {val_accuracy})\")\n",
        "                top_accuracy = val_accuracy\n",
        "        print()"
      ],
      "metadata": {
        "id": "ON9y5_JJOf4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "epochs = 40 # usually train for way longer.\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained = False) # let's initialize a ResNet18 from scratch and train it ourselves\n",
        "model.fc = nn.Linear(in_features=model.fc.in_features, out_features=10, bias=True)\n",
        "\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam is an improved gradient descent algorithm\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBHO5fQgUyYA",
        "outputId": "166b9420-9198-4451-bc27-41500c5dd4cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_training_fixmatch(\n",
        "    model,\n",
        "    labeled_trainloader = cifar_labeled_trainloader,\n",
        "    unlabeled_trainloader = cifar_unlabeled_trainloader,\n",
        "    testloader = cifar_testloader,\n",
        "    optimizer = optimizer,\n",
        "    num_epochs = epochs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oFgPflpLVV40",
        "outputId": "f638a715-f1d2-4f54-9681-1206d4345630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using GPU: Tesla T4\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:47<00:00,  1.06s/it, ConsistencyLoss=0, Epoch=0, FractionMasked=1, LabeledLoss=1.51]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 34.99it/s, Epoch=0, Valid_Acc=41.6, Valid_Loss=1.62]\n",
            "Validation Accuracy Improved (0.0 ---> 41.57)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:46<00:00,  1.05s/it, ConsistencyLoss=0.00187, Epoch=1, FractionMasked=0.985, LabeledLoss=1.72]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 35.45it/s, Epoch=1, Valid_Acc=44.3, Valid_Loss=1.54]\n",
            "Validation Accuracy Improved (41.57 ---> 44.32)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:46<00:00,  1.06s/it, ConsistencyLoss=0.0181, Epoch=2, FractionMasked=0.965, LabeledLoss=1.36]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 34.31it/s, Epoch=2, Valid_Acc=47.2, Valid_Loss=1.47]\n",
            "Validation Accuracy Improved (44.32 ---> 47.21)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:46<00:00,  1.05s/it, ConsistencyLoss=0.0295, Epoch=3, FractionMasked=0.91, LabeledLoss=1.28]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 32.80it/s, Epoch=3, Valid_Acc=49.9, Valid_Loss=1.4]\n",
            "Validation Accuracy Improved (47.21 ---> 49.93)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:46<00:00,  1.05s/it, ConsistencyLoss=0.0239, Epoch=4, FractionMasked=0.93, LabeledLoss=1.26]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 32.04it/s, Epoch=4, Valid_Acc=52.4, Valid_Loss=1.34]\n",
            "Validation Accuracy Improved (49.93 ---> 52.4)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:50<00:00,  1.09s/it, ConsistencyLoss=0.101, Epoch=5, FractionMasked=0.885, LabeledLoss=1.55]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 34.40it/s, Epoch=5, Valid_Acc=49.5, Valid_Loss=1.47]\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:46<00:00,  1.06s/it, ConsistencyLoss=0.0812, Epoch=6, FractionMasked=0.85, LabeledLoss=0.852]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 35.71it/s, Epoch=6, Valid_Acc=57.6, Valid_Loss=1.23]\n",
            "Validation Accuracy Improved (52.4 ---> 57.65)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:46<00:00,  1.05s/it, ConsistencyLoss=0.085, Epoch=7, FractionMasked=0.84, LabeledLoss=1.37]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 35.45it/s, Epoch=7, Valid_Acc=57.8, Valid_Loss=1.23]\n",
            "Validation Accuracy Improved (57.65 ---> 57.84)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:46<00:00,  1.05s/it, ConsistencyLoss=0.0974, Epoch=8, FractionMasked=0.86, LabeledLoss=1.27]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 36.65it/s, Epoch=8, Valid_Acc=57, Valid_Loss=1.28]\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:46<00:00,  1.05s/it, ConsistencyLoss=0.0699, Epoch=9, FractionMasked=0.8, LabeledLoss=1.26]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 35.71it/s, Epoch=9, Valid_Acc=58.7, Valid_Loss=1.24]\n",
            "Validation Accuracy Improved (57.84 ---> 58.74)\n",
            "\n",
            "100%|\u001b[36m██████████\u001b[0m| 101/101 [01:46<00:00,  1.06s/it, ConsistencyLoss=0.0636, Epoch=10, FractionMasked=0.82, LabeledLoss=0.927]\n",
            "100%|\u001b[36m██████████\u001b[0m| 157/157 [00:04<00:00, 36.03it/s, Epoch=10, Valid_Acc=62.2, Valid_Loss=1.09]\n",
            "Validation Accuracy Improved (58.74 ---> 62.24)\n",
            "\n",
            " 14%|\u001b[36m█▍        \u001b[0m| 14/101 [00:15<01:36,  1.11s/it, ConsistencyLoss=0.1, Epoch=11, FractionMasked=0.763, LabeledLoss=0.847]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-eb8e10e52cdd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m run_training_fixmatch(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlabeled_trainloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcifar_labeled_trainloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0munlabeled_trainloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcifar_unlabeled_trainloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtestloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcifar_testloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-81d6bd15b85e>\u001b[0m in \u001b[0;36mrun_training_fixmatch\u001b[0;34m(model, labeled_trainloader, unlabeled_trainloader, testloader, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fixmatch_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeled_trainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlabeled_trainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-81d6bd15b85e>\u001b[0m in \u001b[0;36mtrain_fixmatch_epoch\u001b[0;34m(model, labeled_dataloader, unlabeled_dataloader, device, optimizer, epoch)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0munlabeled_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0munlabeled_images_weak\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlabeled_images_strong\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munlabeled_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-ef1999f79240>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-ef1999f79240>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mweak\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mstrong\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfn_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfn_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfn_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbrightness_factor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_brightness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrightness_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mfn_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontrast_factor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_contrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrast_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36madjust_brightness\u001b[0;34m(img, brightness_factor)\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjust_brightness\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_brightness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrightness_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_brightness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrightness_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36madjust_brightness\u001b[0;34m(img, brightness_factor)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be PIL Image. Got {type(img)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0menhancer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageEnhance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBrightness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menhancer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menhance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrightness_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/ImageEnhance.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegenerate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"A\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mnew\u001b[0;34m(mode, size, color)\u001b[0m\n\u001b[1;32m   2703\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImagePalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImagePalette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2704\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2705\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36m_new\u001b[0;34m(self, im)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m                 \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImagePalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImagePalette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T_sm1TXLVs7G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}